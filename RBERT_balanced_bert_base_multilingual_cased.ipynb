{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "name": "RBERT_balanced-bert-base-multilingual-cased.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "csTFiFHI62nw"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "import os\n",
        "import logging\n",
        "import random\n",
        "import copy\n",
        "import json\n",
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertTokenizer,AdamW, BertConfig, get_linear_schedule_with_warmup\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import BertModel, BertPreTrainedModel\n",
        "from tqdm import tqdm, trange\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygqySK5g62ny"
      },
      "source": [
        "label_path = '../data/label.txt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VRzypx162ny"
      },
      "source": [
        "Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sa9c3QwW62ny"
      },
      "source": [
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "ADDITIONAL_SPECIAL_TOKENS = [\"<e1>\", \"</e1>\", \"<e2>\", \"</e2>\"]\n",
        "\n",
        "\n",
        "def get_label(args):\n",
        "    return [label.strip() for label in open(label_path, \"r\", encoding=\"utf-8\")]\n",
        "\n",
        "\n",
        "def load_tokenizer(args):\n",
        "    tokenizer = BertTokenizer.from_pretrained(args.model_name_or_path)\n",
        "    tokenizer.add_special_tokens({\"additional_special_tokens\": ADDITIONAL_SPECIAL_TOKENS})\n",
        "    return tokenizer\n",
        "\n",
        "\n",
        "def write_prediction(args, output_file, preds):\n",
        "    \"\"\"\n",
        "    For official evaluation script\n",
        "    :param output_file: prediction_file_path (e.g. eval/proposed_answers.txt)\n",
        "    :param preds: [0,1,0,2,18,...]\n",
        "    \"\"\"\n",
        "    relation_labels = get_label(args)\n",
        "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        for idx, pred in enumerate(preds):\n",
        "            f.write(\"{}\\t{}\\n\".format(8001 + idx, relation_labels[pred]))\n",
        "\n",
        "\n",
        "def init_logger():\n",
        "    logging.basicConfig(\n",
        "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
        "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "        level=logging.INFO)\n",
        "\n",
        "\n",
        "def set_seed(args):\n",
        "    random.seed(args.seed)\n",
        "    np.random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "    if not args.no_cuda and torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(args.seed)\n",
        "\n",
        "\n",
        "\n",
        "def compute_metrics(preds, labels):\n",
        "    assert len(preds) == len(labels)\n",
        "    return acc(preds, labels)\n",
        "\n",
        "\n",
        "def simple_accuracy(preds, labels):\n",
        "    return (preds == labels).mean()\n",
        "\n",
        "\n",
        "def acc(preds, labels, average=\"macro\"):\n",
        "    acc = simple_accuracy(preds, labels)\n",
        "    return {\n",
        "        \"acc\": acc}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPpIj9Kr62nz"
      },
      "source": [
        "Data_loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3ehUA6762nz"
      },
      "source": [
        "#from utils import get_label\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class InputExample(object):\n",
        "    \"\"\"\n",
        "    A single training/test example for simple sequence classification.\n",
        "    Args:\n",
        "        guid: Unique id for the example.\n",
        "        text_a: string. The untokenized text of the first sequence. For single\n",
        "        sequence tasks, only this sequence must be specified.\n",
        "        label: (Optional) string. The label of the example. This should be\n",
        "        specified for train and dev examples, but not for test examples.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, guid, text_a, label):\n",
        "        self.guid = guid\n",
        "        self.text_a = text_a\n",
        "        self.label = label\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str(self.to_json_string())\n",
        "\n",
        "    def to_dict(self):\n",
        "        \"\"\"Serializes this instance to a Python dictionary.\"\"\"\n",
        "        output = copy.deepcopy(self.__dict__)\n",
        "        return output\n",
        "\n",
        "    def to_json_string(self):\n",
        "        \"\"\"Serializes this instance to a JSON string.\"\"\"\n",
        "        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\"\n",
        "\n",
        "\n",
        "class InputFeatures(object):\n",
        "    \"\"\"\n",
        "    A single set of features of data.\n",
        "    Args:\n",
        "        input_ids: Indices of input sequence tokens in the vocabulary.\n",
        "        attention_mask: Mask to avoid performing attention on padding token indices.\n",
        "            Mask values selected in ``[0, 1]``:\n",
        "            Usually  ``1`` for tokens that are NOT MASKED, ``0`` for MASKED (padded) tokens.\n",
        "        token_type_ids: Segment token indices to indicate first and second portions of the inputs.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_ids, attention_mask, token_type_ids, label_id, e1_mask, e2_mask):\n",
        "        self.input_ids = input_ids\n",
        "        self.attention_mask = attention_mask\n",
        "        self.token_type_ids = token_type_ids\n",
        "        self.label_id = label_id\n",
        "        self.e1_mask = e1_mask\n",
        "        self.e2_mask = e2_mask\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str(self.to_json_string())\n",
        "\n",
        "    def to_dict(self):\n",
        "        \"\"\"Serializes this instance to a Python dictionary.\"\"\"\n",
        "        output = copy.deepcopy(self.__dict__)\n",
        "        return output\n",
        "\n",
        "    def to_json_string(self):\n",
        "        \"\"\"Serializes this instance to a JSON string.\"\"\"\n",
        "        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\"\n",
        "\n",
        "\n",
        "class SemEvalProcessor(object):\n",
        "    \"\"\"Processor for the Semeval data set \"\"\"\n",
        "\n",
        "    def __init__(self, args):\n",
        "        self.args = args\n",
        "        self.relation_labels = get_label(args)\n",
        "\n",
        "    @classmethod\n",
        "    def _read_tsv(cls, input_file, quotechar=None):\n",
        "        \"\"\"Reads a tab separated value file.\"\"\"\n",
        "        with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
        "            reader = csv.reader(f, delimiter=\"\\t\", quotechar=quotechar)\n",
        "            lines = []\n",
        "            for line in reader:\n",
        "                lines.append(line)\n",
        "            return lines\n",
        "\n",
        "    def _create_examples(self, lines, set_type):\n",
        "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
        "        examples = []\n",
        "        for (i, line) in enumerate(lines):\n",
        "            guid = \"%s-%s\" % (set_type, i)\n",
        "            text_a = line[0]\n",
        "            label = self.relation_labels.index(line[1])\n",
        "            if i % 1000 == 0:\n",
        "                logger.info(line)\n",
        "            examples.append(InputExample(guid=guid, text_a=text_a, label=label))\n",
        "        return examples\n",
        "\n",
        "\n",
        "    def get_examples(self, mode):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            mode: train, dev, test\n",
        "        \"\"\"\n",
        "        file_to_read = None\n",
        "        if mode == \"train_file\":\n",
        "            file_to_read = self.args.train_file\n",
        "        elif mode == \"eval_file\":\n",
        "            file_to_read = self.args.test_file\n",
        "        elif mode == \"test_file\":\n",
        "            file_to_read = self.args.test_file\n",
        "\n",
        "        logger.info(\"LOOKING AT {}\".format(os.path.join(self.args.data_dir, file_to_read)))\n",
        "        return self._create_examples(self._read_tsv(os.path.join(self.args.data_dir, file_to_read)), mode)\n",
        "\n",
        "\n",
        "processors = {\"semeval\": SemEvalProcessor}\n",
        "\n",
        "\n",
        "def read_examples_from_file(data_dir, mode):\n",
        "    file_path = os.path.join(data_dir, \"{}.txt\".format(mode))\n",
        "    guid_index = 1\n",
        "    examples = []\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f.readlines():\n",
        "            line = line.strip().split(\"\\t\")\n",
        "            if len(line) == 2:\n",
        "                text_a = line[0]\n",
        "                label = line[1]\n",
        "            else:\n",
        "                text_a = line[0]\n",
        "                label = \"NONE\"\n",
        "            examples.append(InputExample(guid=guid_index, text_a=text_a, label=label))\n",
        "            guid_index += 1\n",
        "\n",
        "    return examples\n",
        "\n",
        "def convert_examples_to_features(\n",
        "    examples,\n",
        "    max_seq_len,\n",
        "    tokenizer,\n",
        "    cls_token=\"[CLS]\",\n",
        "    cls_token_segment_id=0,\n",
        "    sep_token=\"[SEP]\",\n",
        "    pad_token=0,\n",
        "    pad_token_segment_id=0,\n",
        "    sequence_a_segment_id=0,\n",
        "    add_sep_token=False,\n",
        "    mask_padding_with_zero=True,\n",
        "):\n",
        "    features = []\n",
        "    for (ex_index, example) in enumerate(examples):\n",
        "        if ex_index % 5000 == 0:\n",
        "            logger.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n",
        "\n",
        "        tokens_a = tokenizer.tokenize(example.text_a)\n",
        "        print(tokens_a)\n",
        "\n",
        "        e11_p = tokens_a.index(\"<e1>\")  # the start position of entity1\n",
        "        e12_p = tokens_a.index(\"</e1>\")  # the end position of entity1\n",
        "        e21_p = tokens_a.index(\"<e2>\")  # the start position of entity2\n",
        "        e22_p = tokens_a.index(\"</e2>\")  # the end position of entity2\n",
        "\n",
        "        # Replace the token\n",
        "        tokens_a[e11_p] = \"$\"\n",
        "        tokens_a[e12_p] = \"$\"\n",
        "        tokens_a[e21_p] = \"#\"\n",
        "        tokens_a[e22_p] = \"#\"\n",
        "\n",
        "        # Add 1 because of the [CLS] token\n",
        "        e11_p += 1\n",
        "        e12_p += 1\n",
        "        e21_p += 1\n",
        "        e22_p += 1\n",
        "\n",
        "        # Account for [CLS] and [SEP] with \"- 2\" and with \"- 3\" for RoBERTa.\n",
        "        if add_sep_token:\n",
        "            special_tokens_count = 2\n",
        "        else:\n",
        "            special_tokens_count = 1\n",
        "        if len(tokens_a) > max_seq_len - special_tokens_count:\n",
        "            tokens_a = tokens_a[: (max_seq_len - special_tokens_count)]\n",
        "\n",
        "        tokens = tokens_a\n",
        "        if add_sep_token:\n",
        "            tokens += [sep_token]\n",
        "\n",
        "        token_type_ids = [sequence_a_segment_id] * len(tokens)\n",
        "\n",
        "        tokens = [cls_token] + tokens\n",
        "        token_type_ids = [cls_token_segment_id] + token_type_ids\n",
        "\n",
        "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "        # The mask has 1 for real tokens and 0 for padding tokens. Only real tokens are attended to.\n",
        "        attention_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n",
        "\n",
        "        # Zero-pad up to the sequence length.\n",
        "        padding_length = max_seq_len - len(input_ids)\n",
        "        input_ids = input_ids + ([pad_token] * padding_length)\n",
        "        attention_mask = attention_mask + ([0 if mask_padding_with_zero else 1] * padding_length)\n",
        "        token_type_ids = token_type_ids + ([pad_token_segment_id] * padding_length)\n",
        "\n",
        "        # e1 mask, e2 mask\n",
        "        e1_mask = [0] * len(attention_mask)\n",
        "        e2_mask = [0] * len(attention_mask)\n",
        "\n",
        "        for i in range(e11_p, e12_p + 1):\n",
        "            e1_mask[i] = 1\n",
        "        for i in range(e21_p, e22_p + 1):\n",
        "            e2_mask[i] = 1\n",
        "\n",
        "        assert len(input_ids) == max_seq_len, \"Error with input length {} vs {}\".format(len(input_ids), max_seq_len)\n",
        "        assert len(attention_mask) == max_seq_len, \"Error with attention mask length {} vs {}\".format(\n",
        "            len(attention_mask), max_seq_len\n",
        "        )\n",
        "        assert len(token_type_ids) == max_seq_len, \"Error with token type length {} vs {}\".format(\n",
        "            len(token_type_ids), max_seq_len\n",
        "        )\n",
        "\n",
        "        label_id = int(example.label)\n",
        "\n",
        "        if ex_index < 5:\n",
        "            logger.info(\"*** Example ***\")\n",
        "            logger.info(\"guid: %s\" % example.guid)\n",
        "            logger.info(\"tokens: %s\" % \" \".join([str(x) for x in tokens]))\n",
        "            logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
        "            logger.info(\"attention_mask: %s\" % \" \".join([str(x) for x in attention_mask]))\n",
        "            logger.info(\"token_type_ids: %s\" % \" \".join([str(x) for x in token_type_ids]))\n",
        "            logger.info(\"label: %s (id = %d)\" % (example.label, label_id))\n",
        "            logger.info(\"e1_mask: %s\" % \" \".join([str(x) for x in e1_mask]))\n",
        "            logger.info(\"e2_mask: %s\" % \" \".join([str(x) for x in e2_mask]))\n",
        "\n",
        "        features.append(\n",
        "            InputFeatures(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                token_type_ids=token_type_ids,\n",
        "                label_id=label_id,\n",
        "                e1_mask=e1_mask,\n",
        "                e2_mask=e2_mask,\n",
        "            )\n",
        "        )\n",
        "\n",
        "    return features\n",
        "\n",
        "\n",
        "def load_and_cache_examples(args, tokenizer, mode):\n",
        "    processor = processors[args.task](args)\n",
        "\n",
        "    # Load data features from cache or dataset file\n",
        "    cached_features_file = os.path.join(\n",
        "        args.data_dir,\n",
        "        \"cached_{}_{}_{}_{}\".format(\n",
        "            mode,\n",
        "            args.task,\n",
        "            list(filter(None, args.model_name_or_path.split(\"/\"))).pop(),\n",
        "            args.max_seq_len,\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    if os.path.exists(cached_features_file):\n",
        "        logger.info(\"Loading features from cached file %s\", cached_features_file)\n",
        "        features = torch.load(cached_features_file)\n",
        "    else:\n",
        "        logger.info(\"Creating features from dataset file at %s\", args.data_dir)\n",
        "        if mode == \"train_file\":\n",
        "            examples = processor.get_examples(\"train_file\")\n",
        "        elif mode == \"eval_file\":\n",
        "            examples = processor.get_examples(\"eval_file\")\n",
        "        elif mode == \"test_file\":\n",
        "            examples = processor.get_examples(\"test_file\")\n",
        "        else:\n",
        "            raise Exception(\"For mode, Only train, dev, test is available\")\n",
        "\n",
        "        features = convert_examples_to_features(\n",
        "            examples, args.max_seq_len, tokenizer\n",
        "        )\n",
        "        logger.info(\"Saving features into cached file %s\", cached_features_file)\n",
        "        torch.save(features, cached_features_file)\n",
        "\n",
        "    # Convert to Tensors and build dataset\n",
        "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
        "    all_attention_mask = torch.tensor([f.attention_mask for f in features], dtype=torch.long)\n",
        "    all_token_type_ids = torch.tensor([f.token_type_ids for f in features], dtype=torch.long)\n",
        "    all_e1_mask = torch.tensor([f.e1_mask for f in features], dtype=torch.long)  # add e1 mask\n",
        "    all_e2_mask = torch.tensor([f.e2_mask for f in features], dtype=torch.long)  # add e2 mask\n",
        "\n",
        "    all_label_ids = torch.tensor([f.label_id for f in features], dtype=torch.long)\n",
        "\n",
        "    dataset = TensorDataset(\n",
        "        all_input_ids,\n",
        "        all_attention_mask,\n",
        "        all_token_type_ids,\n",
        "        all_label_ids,\n",
        "        all_e1_mask,\n",
        "        all_e2_mask,\n",
        "    )\n",
        "    return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ja4apgnN62n1"
      },
      "source": [
        "Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUEK9L6A62n2"
      },
      "source": [
        "class FCLayer(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, dropout_rate=0.0, use_activation=True):\n",
        "        super(FCLayer, self).__init__()\n",
        "        self.use_activation = use_activation\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.linear = nn.Linear(input_dim, output_dim)\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.dropout(x)\n",
        "        if self.use_activation:\n",
        "            x = self.tanh(x)\n",
        "        return self.linear(x)\n",
        "\n",
        "\n",
        "class RBERT(BertPreTrainedModel):\n",
        "    def __init__(self, config, args):\n",
        "        super(RBERT, self).__init__(config)\n",
        "        self.bert = BertModel(config=config)  # Load pretrained bert\n",
        "\n",
        "        self.num_labels = config.num_labels\n",
        "\n",
        "        self.cls_fc_layer = FCLayer(config.hidden_size, config.hidden_size, args.dropout_rate)\n",
        "        self.entity_fc_layer = FCLayer(config.hidden_size, config.hidden_size, args.dropout_rate)\n",
        "        self.label_classifier = FCLayer(\n",
        "            config.hidden_size * 3,\n",
        "            config.num_labels,\n",
        "            args.dropout_rate,\n",
        "            use_activation=False,\n",
        "        )\n",
        "\n",
        "    @staticmethod\n",
        "    def entity_average(hidden_output, e_mask):\n",
        "        \"\"\"\n",
        "        Average the entity hidden state vectors (H_i ~ H_j)\n",
        "        :param hidden_output: [batch_size, j-i+1, dim]\n",
        "        :param e_mask: [batch_size, max_seq_len]\n",
        "                e.g. e_mask[0] == [0, 0, 0, 1, 1, 1, 0, 0, ... 0]\n",
        "        :return: [batch_size, dim]\n",
        "        \"\"\"\n",
        "        e_mask_unsqueeze = e_mask.unsqueeze(1)  # [b, 1, j-i+1]\n",
        "        length_tensor = (e_mask != 0).sum(dim=1).unsqueeze(1)  # [batch_size, 1]\n",
        "\n",
        "        # [b, 1, j-i+1] * [b, j-i+1, dim] = [b, 1, dim] -> [b, dim]\n",
        "        sum_vector = torch.bmm(e_mask_unsqueeze.float(), hidden_output).squeeze(1)\n",
        "        avg_vector = sum_vector.float() / length_tensor.float()  # broadcasting\n",
        "        return avg_vector\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, token_type_ids, labels, e1_mask, e2_mask):\n",
        "        outputs = self.bert(\n",
        "            input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids\n",
        "        )  # sequence_output, pooled_output, (hidden_states), (attentions)\n",
        "        sequence_output = outputs[0]\n",
        "        pooled_output = outputs[1]  # [CLS]\n",
        "\n",
        "        # Average\n",
        "        e1_h = self.entity_average(sequence_output, e1_mask)\n",
        "        e2_h = self.entity_average(sequence_output, e2_mask)\n",
        "\n",
        "        # Dropout -> tanh -> fc_layer (Share FC layer for e1 and e2)\n",
        "        pooled_output = self.cls_fc_layer(pooled_output)\n",
        "        e1_h = self.entity_fc_layer(e1_h)\n",
        "        e2_h = self.entity_fc_layer(e2_h)\n",
        "\n",
        "        # Concat -> fc_layer\n",
        "        concat_h = torch.cat([pooled_output, e1_h, e2_h], dim=-1)\n",
        "        logits = self.label_classifier(concat_h)\n",
        "\n",
        "        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n",
        "\n",
        "        # Softmax\n",
        "        if labels is not None:\n",
        "            if self.num_labels == 1:\n",
        "                loss_fct = nn.MSELoss()\n",
        "                loss = loss_fct(logits.view(-1), labels.view(-1))\n",
        "            else:\n",
        "                loss_fct = nn.CrossEntropyLoss()\n",
        "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "\n",
        "            outputs = (loss,) + outputs\n",
        "\n",
        "        return outputs  # (loss), logits, (hidden_states), (attentions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qpo_jwb62n3"
      },
      "source": [
        "Trainer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cG_4tjp-62n3"
      },
      "source": [
        "def get_device(pred_config):\n",
        "    return \"cuda\" if torch.cuda.is_available() and not pred_config.no_cuda else \"cpu\"\n",
        "\n",
        "def convert_input_file_to_tensor_dataset(\n",
        "    args,\n",
        "    cls_token_segment_id=0,\n",
        "    pad_token_segment_id=0,\n",
        "    sequence_a_segment_id=0,\n",
        "    mask_padding_with_zero=True):\n",
        "    tokenizer = load_tokenizer(args)\n",
        "\n",
        "    # Setting based on the current model type\n",
        "    cls_token = tokenizer.cls_token\n",
        "    sep_token = tokenizer.sep_token\n",
        "    pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "    all_input_ids = []\n",
        "    all_attention_mask = []\n",
        "    all_token_type_ids = []\n",
        "    all_e1_mask = []\n",
        "    all_e2_mask = []\n",
        "\n",
        "    with open(args.input_file, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            tokens = tokenizer.tokenize(line)\n",
        "\n",
        "            e11_p = tokens.index(\"<e1>\")  # the start position of entity1\n",
        "            e12_p = tokens.index(\"</e1>\")  # the end position of entity1\n",
        "            e21_p = tokens.index(\"<e2>\")  # the start position of entity2\n",
        "            e22_p = tokens.index(\"</e2>\")  # the end position of entity2\n",
        "\n",
        "            # Replace the token\n",
        "            tokens[e11_p] = \"$\"\n",
        "            tokens[e12_p] = \"$\"\n",
        "            tokens[e21_p] = \"#\"\n",
        "            tokens[e22_p] = \"#\"\n",
        "\n",
        "            # Add 1 because of the [CLS] token\n",
        "            e11_p += 1\n",
        "            e12_p += 1\n",
        "            e21_p += 1\n",
        "            e22_p += 1\n",
        "\n",
        "            # Account for [CLS] and [SEP] with \"- 2\" and with \"- 3\" for RoBERTa.\n",
        "            if args.add_sep_token:\n",
        "                special_tokens_count = 2\n",
        "            else:\n",
        "                special_tokens_count = 1\n",
        "            if len(tokens) > args.max_seq_len - special_tokens_count:\n",
        "                tokens = tokens[: (args.max_seq_len - special_tokens_count)]\n",
        "\n",
        "            # Add [SEP] token\n",
        "            if args.add_sep_token:\n",
        "                tokens += [sep_token]\n",
        "            token_type_ids = [sequence_a_segment_id] * len(tokens)\n",
        "\n",
        "            # Add [CLS] token\n",
        "            tokens = [cls_token] + tokens\n",
        "            token_type_ids = [cls_token_segment_id] + token_type_ids\n",
        "\n",
        "            input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "            # The mask has 1 for real tokens and 0 for padding tokens. Only real tokens are attended to.\n",
        "            attention_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n",
        "\n",
        "            # Zero-pad up to the sequence length.\n",
        "            padding_length = args.max_seq_len - len(input_ids)\n",
        "            input_ids = input_ids + ([pad_token_id] * padding_length)\n",
        "            attention_mask = attention_mask + ([0 if mask_padding_with_zero else 1] * padding_length)\n",
        "            token_type_ids = token_type_ids + ([pad_token_segment_id] * padding_length)\n",
        "\n",
        "            # e1 mask, e2 mask\n",
        "            e1_mask = [0] * len(attention_mask)\n",
        "            e2_mask = [0] * len(attention_mask)\n",
        "\n",
        "            for i in range(e11_p, e12_p + 1):\n",
        "                e1_mask[i] = 1\n",
        "            for i in range(e21_p, e22_p + 1):\n",
        "                e2_mask[i] = 1\n",
        "\n",
        "            all_input_ids.append(input_ids)\n",
        "            all_attention_mask.append(attention_mask)\n",
        "            all_token_type_ids.append(token_type_ids)\n",
        "            all_e1_mask.append(e1_mask)\n",
        "            all_e2_mask.append(e2_mask)\n",
        "\n",
        "    # Change to Tensor\n",
        "    all_input_ids = torch.tensor(all_input_ids, dtype=torch.long)\n",
        "    all_attention_mask = torch.tensor(all_attention_mask, dtype=torch.long)\n",
        "    all_token_type_ids = torch.tensor(all_token_type_ids, dtype=torch.long)\n",
        "    all_e1_mask = torch.tensor(all_e1_mask, dtype=torch.long)\n",
        "    all_e2_mask = torch.tensor(all_e2_mask, dtype=torch.long)\n",
        "\n",
        "    dataset = TensorDataset(all_input_ids, all_attention_mask, all_token_type_ids, all_e1_mask, all_e2_mask)\n",
        "\n",
        "    return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UFovWrw62n4"
      },
      "source": [
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "class Trainer(object):\n",
        "    def __init__(self, args, train_dataset=None, dev_dataset=None, test_dataset=None):\n",
        "        self.args = args\n",
        "        self.train_dataset = train_dataset\n",
        "        self.dev_dataset = dev_dataset\n",
        "        self.test_dataset = test_dataset\n",
        "\n",
        "        self.label_lst = get_label(args)\n",
        "        self.num_labels = len(self.label_lst)\n",
        "\n",
        "        self.config = BertConfig.from_pretrained(\n",
        "            args.model_name_or_path,\n",
        "            num_labels=self.num_labels,\n",
        "            finetuning_task=args.task,\n",
        "            id2label={str(i): label for i, label in enumerate(self.label_lst)},\n",
        "            label2id={label: i for i, label in enumerate(self.label_lst)},\n",
        "        )\n",
        "        self.model = RBERT.from_pretrained(args.model_name_or_path, config=self.config, args=args)\n",
        "\n",
        "        # GPU or CPU\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\"\n",
        "        self.model.to(self.device)\n",
        "\n",
        "        \n",
        "    def evaluate(self, mode):\n",
        "        # We use test dataset because semeval doesn't have dev dataset\n",
        "        if mode == \"test\":\n",
        "            dataset = self.test_dataset\n",
        "        elif mode == \"dev\":\n",
        "            dataset = self.dev_dataset\n",
        "        else:\n",
        "            raise Exception(\"Only dev and test dataset available\")\n",
        "\n",
        "        eval_sampler = SequentialSampler(dataset)\n",
        "        eval_dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=self.args.eval_batch_size)\n",
        "        # Eval!\n",
        "\n",
        "        eval_loss = 0.0\n",
        "        nb_eval_steps = 0\n",
        "        preds = None\n",
        "        out_label_ids = None\n",
        "\n",
        "        self.model.eval()\n",
        "\n",
        "        for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
        "            batch = tuple(t.to(self.device) for t in batch)\n",
        "            with torch.no_grad():\n",
        "                inputs = {\n",
        "                    \"input_ids\": batch[0],\n",
        "                    \"attention_mask\": batch[1],\n",
        "                    \"token_type_ids\": batch[2],\n",
        "                    \"labels\": batch[3],\n",
        "                    \"e1_mask\": batch[4],\n",
        "                    \"e2_mask\": batch[5],\n",
        "                }\n",
        "                outputs = self.model(**inputs)\n",
        "                tmp_eval_loss, logits = outputs[:2]\n",
        "\n",
        "                eval_loss += tmp_eval_loss.mean().item()\n",
        "            nb_eval_steps += 1\n",
        "\n",
        "            if preds is None:\n",
        "                preds = logits.detach().cpu().numpy()\n",
        "                out_label_ids = inputs[\"labels\"].detach().cpu().numpy()\n",
        "            else:\n",
        "                preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
        "                out_label_ids = np.append(out_label_ids, inputs[\"labels\"].detach().cpu().numpy(), axis=0)\n",
        "\n",
        "        eval_loss = eval_loss / nb_eval_steps\n",
        "        #results = {\"loss\": eval_loss}\n",
        "        preds = np.argmax(preds, axis=1)\n",
        "        write_prediction(self.args, os.path.join(self.args.eval_dir, \"proposed_answers_multilingual.txt\"), preds)\n",
        "\n",
        "        results = {\"loss\": eval_loss, 'accuracy' : accuracy_score(out_label_ids, preds), \n",
        "                   'f1_score': f1_score(out_label_ids, preds, average='weighted'),\n",
        "                  'roc_auc': roc_auc_score(out_label_ids, preds)}\n",
        "\n",
        "          #result = compute_metrics(preds, out_label_ids)\n",
        "          #results.update(result)\n",
        "\n",
        "        logger.info(\"***** Eval results *****\")\n",
        "        for key in sorted(results.keys()):\n",
        "            logger.info(\"  {} = {:.4f}\".format(key, results[key]))\n",
        "\n",
        "        return results\n",
        "\n",
        "    \n",
        "    def train(self):\n",
        "        train_sampler = RandomSampler(self.train_dataset)\n",
        "        train_dataloader = DataLoader(\n",
        "            self.train_dataset,\n",
        "            sampler=train_sampler,\n",
        "            batch_size=self.args.train_batch_size,\n",
        "        )\n",
        "\n",
        "        if self.args.max_steps > 0:\n",
        "            t_total = self.args.max_steps\n",
        "            self.args.num_train_epochs = (\n",
        "                self.args.max_steps // (len(train_dataloader) // self.args.gradient_accumulation_steps) + 1\n",
        "            )\n",
        "        else:\n",
        "            t_total = len(train_dataloader) // self.args.gradient_accumulation_steps * self.args.num_train_epochs\n",
        "\n",
        "        # Prepare optimizer and schedule (linear warmup and decay)\n",
        "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "        optimizer_grouped_parameters = [\n",
        "            {\n",
        "                \"params\": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "                \"weight_decay\": self.args.weight_decay,\n",
        "            },\n",
        "            {\n",
        "                \"params\": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],\n",
        "                \"weight_decay\": 0.0,\n",
        "            },\n",
        "        ]\n",
        "        optimizer = AdamW(\n",
        "            optimizer_grouped_parameters,\n",
        "            lr=self.args.learning_rate,\n",
        "            eps=self.args.adam_epsilon,\n",
        "        )\n",
        "        scheduler = get_linear_schedule_with_warmup(\n",
        "            optimizer,\n",
        "            num_warmup_steps=self.args.warmup_steps,\n",
        "            num_training_steps=t_total,\n",
        "        )\n",
        "        # Train!\n",
        "\n",
        "        global_step = 0\n",
        "        tr_loss = 0.0\n",
        "        self.model.zero_grad()\n",
        "\n",
        "        train_iterator = trange(int(self.args.num_train_epochs), desc=\"Epoch\")\n",
        "\n",
        "        for _ in train_iterator:\n",
        "            epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\")\n",
        "            for step, batch in enumerate(epoch_iterator):\n",
        "                self.model.train()\n",
        "                batch = tuple(t.to(self.device) for t in batch)  # GPU or CPU\n",
        "                inputs = {\n",
        "                    \"input_ids\": batch[0],\n",
        "                    \"attention_mask\": batch[1],\n",
        "                    \"token_type_ids\": batch[2],\n",
        "                    \"labels\": batch[3],\n",
        "                    \"e1_mask\": batch[4],\n",
        "                    \"e2_mask\": batch[5],\n",
        "                }\n",
        "                outputs = self.model(**inputs)\n",
        "                loss = outputs[0]\n",
        "\n",
        "                if self.args.gradient_accumulation_steps > 1:\n",
        "                    loss = loss / self.args.gradient_accumulation_steps\n",
        "\n",
        "                loss.backward()\n",
        "\n",
        "                tr_loss += loss.item()\n",
        "                if (step + 1) % self.args.gradient_accumulation_steps == 0:\n",
        "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.args.max_grad_norm)\n",
        "\n",
        "                    optimizer.step()\n",
        "                    scheduler.step()  # Update learning rate schedule\n",
        "                    self.model.zero_grad()\n",
        "                    global_step += 1\n",
        "\n",
        "            print(\"\\n====Evaluation====\")\n",
        "            print(\"\\nEvaluation: \", self.evaluate(\"test\"))\n",
        "            \n",
        "        self.save_model(self.model)\n",
        "\n",
        "    def save_model(self, model):\n",
        "        torch.save(model.state_dict(), 'model/model_multilingual.bin')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAcRiNGr62n5"
      },
      "source": [
        "def load_saved_model(args):\n",
        "    config = BertConfig.from_pretrained(args.model_name_or_path, num_labels = args.num_labels)\n",
        "    model = RBERT.from_pretrained('model/model_multilingual.bin', config=config, args=args)\n",
        "    model.to(\"cpu\")\n",
        "    return model\n",
        "\n",
        "def predict(pred_config):\n",
        "        device = \"cpu\"\n",
        "        model = load_saved_model(pred_config)\n",
        "        tokenizer = load_tokenizer(pred_config)\n",
        "\n",
        "        # Convert input file to TensorDataset\n",
        "        dataset = convert_input_file_to_tensor_dataset(pred_config)\n",
        "\n",
        "        # Predict\n",
        "        sampler = SequentialSampler(dataset)\n",
        "        data_loader = DataLoader(dataset, sampler=sampler, batch_size=pred_config.batch_size)\n",
        "\n",
        "        preds = None\n",
        "\n",
        "\n",
        "        for batch in tqdm(data_loader, desc=\"Predicting\"):\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            with torch.no_grad():\n",
        "                inputs = {\n",
        "                    \"input_ids\": batch[0],\n",
        "                    \"attention_mask\": batch[1],\n",
        "                    \"token_type_ids\": batch[2],\n",
        "                    \"labels\": None,\n",
        "                    \"e1_mask\": batch[3],\n",
        "                    \"e2_mask\": batch[4],\n",
        "                }\n",
        "                outputs = model(**inputs)\n",
        "                logits = outputs[0]\n",
        "\n",
        "                if preds is None:\n",
        "                    preds = logits.detach().cpu().numpy()\n",
        "                else:\n",
        "                    preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
        "\n",
        "        preds = np.argmax(preds, axis=1)\n",
        "\n",
        "        # Write to output file\n",
        "        label_lst = get_label(pred_config)\n",
        "        with open(pred_config.output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "            for pred in preds:\n",
        "                f.write(\"{}\\n\".format(label_lst[pred]))\n",
        "\n",
        "        print('Prediction was done')\n",
        "        return preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oG-1slhV62n5"
      },
      "source": [
        "Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0s7lDHYF62n5"
      },
      "source": [
        "def RBERT_re(args):\n",
        "    set_seed(args)\n",
        "    tokenizer = load_tokenizer(args)\n",
        "\n",
        "    train_dataset = load_and_cache_examples(args, tokenizer, mode=\"train_file\")\n",
        "    test_dataset = load_and_cache_examples(args, tokenizer, mode=\"eval_file\")\n",
        "\n",
        "    trainer = Trainer(args, train_dataset=train_dataset, test_dataset=test_dataset)\n",
        "\n",
        "\n",
        "    if args.do_train:\n",
        "        trainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KYMBJH162n6"
      },
      "source": [
        "class Trainer_args(object):\n",
        "    def __init__(self,\n",
        "                model_name_or_path = 'bert-base-multilingual-cased',\n",
        "                seed = 24,\n",
        "                task = \"semeval\",\n",
        "                train_file = 'train_balanced.csv', \n",
        "                test_file = 'eval_balanced.csv',\n",
        "                label_file = 'label.txt',  \n",
        "                dropout_rate = 0.1,\n",
        "                num_labels = 2,\n",
        "                learning_rate = 2e-5,\n",
        "                num_train_epochs = 12,\n",
        "                max_seq_len = 384,\n",
        "                train_batch_size = 16,\n",
        "                eval_batch_size = 16,\n",
        "                adam_epsilon = 1e-8,\n",
        "                gradient_accumulation_steps = 1,\n",
        "                max_grad_norm = 1.0,\n",
        "                logging_steps = 250,\n",
        "                save_steps = 250,\n",
        "                weight_decay = 0.0,\n",
        "                add_sep_token = True,\n",
        "                do_train = True,\n",
        "                no_cuda = True,\n",
        "                do_eval = True,\n",
        "                max_steps = -1,\n",
        "                warmup_steps = 0,\n",
        "                model_dir = 'model/',\n",
        "                data_dir = '../data/',\n",
        "                eval_dir = '../data/'\n",
        "                ):\n",
        "\n",
        "        super(Trainer_args, self).__init__()\n",
        "\n",
        "        self.train_file = train_file\n",
        "        self.test_file = test_file\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.num_labels = num_labels\n",
        "        self.learning_rate = learning_rate\n",
        "        self.num_train_epochs = num_train_epochs\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.train_batch_size = train_batch_size\n",
        "        self.adam_epsilon = adam_epsilon\n",
        "        self.gradient_accumulation_steps = gradient_accumulation_steps\n",
        "        self.max_grad_norm = max_grad_norm\n",
        "        self.logging_steps = logging_steps\n",
        "        self.save_steps = save_steps\n",
        "        self.weight_decay = weight_decay\n",
        "        self.data_dir = data_dir\n",
        "        self.model_name_or_path = model_name_or_path\n",
        "        self.seed = seed\n",
        "        self.task = task\n",
        "        self.add_sep_token = add_sep_token\n",
        "        self.do_train = do_train\n",
        "        self.no_cuda = no_cuda\n",
        "        self.max_steps = max_steps\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.model_dir = model_dir\n",
        "        self.label_file = label_file\n",
        "        self.eval_batch_size = eval_batch_size\n",
        "        self.do_eval = do_eval\n",
        "        self.eval_dir = eval_dir\n",
        "        return \n",
        "args = Trainer_args()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUE8oLT762n6"
      },
      "source": [
        "train_path = '../data/train_balanced.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caOuFyIe62n7",
        "outputId": "02fdb8c2-7eeb-4859-fe12-3ecdc0c611cb"
      },
      "source": [
        "main_model = RBERT_re(args)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing RBERT: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing RBERT from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RBERT from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RBERT were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['cls_fc_layer.linear.weight', 'cls_fc_layer.linear.bias', 'entity_fc_layer.linear.weight', 'entity_fc_layer.linear.bias', 'label_classifier.linear.weight', 'label_classifier.linear.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Epoch:   0%|                                            | 0/12 [00:00<?, ?it/s]\n",
            "Iteration:   0%|                                         | 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:  11%|███▎                          | 1/9 [07:48<1:02:26, 468.29s/it]\u001b[A\n",
            "Iteration:  22%|███████                         | 2/9 [09:44<42:18, 362.64s/it]\u001b[A\n",
            "Iteration:  33%|██████████▋                     | 3/9 [11:53<29:12, 292.16s/it]\u001b[A\n",
            "Iteration:  44%|██████████████▏                 | 4/9 [13:42<19:47, 237.57s/it]\u001b[A\n",
            "Iteration:  56%|█████████████████▊              | 5/9 [15:46<13:33, 203.35s/it]\u001b[A\n",
            "Iteration:  67%|█████████████████████▎          | 6/9 [17:32<08:43, 174.36s/it]\u001b[A\n",
            "Iteration:  78%|████████████████████████▉       | 7/9 [19:36<05:16, 158.49s/it]\u001b[A\n",
            "Iteration:  89%|████████████████████████████▍   | 8/9 [21:29<02:25, 145.51s/it]\u001b[A\n",
            "Iteration: 100%|████████████████████████████████| 9/9 [22:54<00:00, 152.76s/it]\u001b[A\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====Evaluation====\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating:   0%|                                        | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating:  12%|████                            | 1/8 [00:40<04:45, 40.73s/it]\u001b[A\n",
            "Evaluating:  25%|████████                        | 2/8 [01:20<04:02, 40.37s/it]\u001b[A\n",
            "Evaluating:  38%|████████████                    | 3/8 [01:58<03:18, 39.73s/it]\u001b[A\n",
            "Evaluating:  50%|████████████████                | 4/8 [02:38<02:39, 39.81s/it]\u001b[A\n",
            "Evaluating:  62%|████████████████████            | 5/8 [03:18<01:59, 39.78s/it]\u001b[A\n",
            "Evaluating:  75%|████████████████████████        | 6/8 [03:56<01:18, 39.40s/it]\u001b[A\n",
            "Evaluating:  88%|████████████████████████████    | 7/8 [04:35<00:39, 39.09s/it]\u001b[A\n",
            "Evaluating: 100%|████████████████████████████████| 8/8 [05:01<00:00, 37.70s/it]\u001b[A\n",
            "Epoch:   8%|██▋                             | 1/12 [28:00<5:08:00, 1680.04s/it]\n",
            "Iteration:   0%|                                         | 0/9 [00:00<?, ?it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluation:  {'loss': 0.6837839782238007, 'accuracy': 0.5853658536585366, 'f1_score': 0.6247049567269867, 'roc_auc': 0.6950757575757576}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration:  11%|███▌                            | 1/9 [01:50<14:47, 110.97s/it]\u001b[A\n",
            "Iteration:  22%|███████                         | 2/9 [03:49<13:12, 113.26s/it]\u001b[A\n",
            "Iteration:  33%|██████████▋                     | 3/9 [05:43<11:20, 113.41s/it]\u001b[A\n",
            "Iteration:  44%|██████████████▏                 | 4/9 [07:43<09:36, 115.24s/it]\u001b[A\n",
            "Iteration:  56%|█████████████████▊              | 5/9 [09:37<07:39, 115.00s/it]\u001b[A\n",
            "Iteration:  67%|█████████████████████▎          | 6/9 [11:38<05:50, 116.95s/it]\u001b[A\n",
            "Iteration:  78%|████████████████████████▉       | 7/9 [13:41<03:57, 118.67s/it]\u001b[A\n",
            "Iteration:  89%|████████████████████████████▍   | 8/9 [15:43<01:59, 119.51s/it]\u001b[A\n",
            "Iteration: 100%|████████████████████████████████| 9/9 [17:06<00:00, 114.11s/it]\u001b[A\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====Evaluation====\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating:   0%|                                        | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating:  12%|████                            | 1/8 [00:40<04:44, 40.59s/it]\u001b[A\n",
            "Evaluating:  25%|████████                        | 2/8 [01:19<04:00, 40.15s/it]\u001b[A\n",
            "Evaluating:  38%|████████████                    | 3/8 [01:58<03:18, 39.70s/it]\u001b[A\n",
            "Evaluating:  50%|████████████████                | 4/8 [02:36<02:37, 39.34s/it]\u001b[A\n",
            "Evaluating:  62%|████████████████████            | 5/8 [03:15<01:57, 39.20s/it]\u001b[A\n",
            "Evaluating:  75%|████████████████████████        | 6/8 [03:54<01:17, 38.99s/it]\u001b[A\n",
            "Evaluating:  88%|████████████████████████████    | 7/8 [04:32<00:38, 38.91s/it]\u001b[A\n",
            "Evaluating: 100%|████████████████████████████████| 8/8 [05:00<00:00, 37.54s/it]\u001b[A\n",
            "Epoch:  17%|█████▎                          | 2/12 [50:10<4:22:31, 1575.15s/it]\n",
            "Iteration:   0%|                                         | 0/9 [00:00<?, ?it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluation:  {'loss': 0.6275632604956627, 'accuracy': 0.6747967479674797, 'f1_score': 0.7085068411659727, 'roc_auc': 0.7506313131313131}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration:  11%|███▌                            | 1/9 [01:51<14:51, 111.45s/it]\u001b[A\n",
            "Iteration:  22%|███████                         | 2/9 [03:44<13:02, 111.83s/it]\u001b[A\n",
            "Iteration:  33%|██████████▋                     | 3/9 [05:37<11:13, 112.25s/it]\u001b[A\n",
            "Iteration:  44%|██████████████▏                 | 4/9 [07:30<09:22, 112.54s/it]\u001b[A\n",
            "Iteration:  56%|█████████████████▊              | 5/9 [09:22<07:29, 112.26s/it]\u001b[A\n",
            "Iteration:  67%|█████████████████████▎          | 6/9 [11:14<05:36, 112.17s/it]\u001b[A\n",
            "Iteration:  78%|████████████████████████▉       | 7/9 [13:06<03:44, 112.19s/it]\u001b[A\n",
            "Iteration:  89%|████████████████████████████▍   | 8/9 [15:02<01:53, 113.35s/it]\u001b[A\n",
            "Iteration: 100%|████████████████████████████████| 9/9 [16:27<00:00, 109.68s/it]\u001b[A\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====Evaluation====\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating:   0%|                                        | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating:  12%|████                            | 1/8 [00:39<04:39, 39.90s/it]\u001b[A\n",
            "Evaluating:  25%|████████                        | 2/8 [01:19<03:58, 39.77s/it]\u001b[A\n",
            "Evaluating:  38%|████████████                    | 3/8 [01:57<03:16, 39.30s/it]\u001b[A\n",
            "Evaluating:  50%|████████████████                | 4/8 [02:35<02:35, 38.95s/it]\u001b[A\n",
            "Evaluating:  62%|████████████████████            | 5/8 [03:13<01:56, 38.73s/it]\u001b[A\n",
            "Evaluating:  75%|████████████████████████        | 6/8 [03:52<01:17, 38.65s/it]\u001b[A\n",
            "Evaluating:  88%|████████████████████████████    | 7/8 [04:32<00:39, 39.12s/it]\u001b[A\n",
            "Evaluating: 100%|████████████████████████████████| 8/8 [05:00<00:00, 37.62s/it]\u001b[A\n",
            "Epoch:  25%|███████▌                      | 3/12 [1:11:41<3:43:28, 1489.80s/it]\n",
            "Iteration:   0%|                                         | 0/9 [00:00<?, ?it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluation:  {'loss': 0.5203968472778797, 'accuracy': 0.7804878048780488, 'f1_score': 0.7988846620297907, 'roc_auc': 0.7847222222222221}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration:  11%|███▌                            | 1/9 [01:50<14:43, 110.40s/it]\u001b[A\n",
            "Iteration:  22%|███████                         | 2/9 [03:44<13:00, 111.54s/it]\u001b[A\n",
            "Iteration:  33%|██████████▋                     | 3/9 [05:36<11:10, 111.70s/it]\u001b[A\n",
            "Iteration:  44%|██████████████▏                 | 4/9 [07:29<09:20, 112.08s/it]\u001b[A\n",
            "Iteration:  56%|█████████████████▊              | 5/9 [09:26<07:34, 113.53s/it]\u001b[A\n",
            "Iteration:  67%|█████████████████████▎          | 6/9 [11:20<05:40, 113.49s/it]\u001b[A\n",
            "Iteration:  78%|████████████████████████▉       | 7/9 [13:10<03:45, 112.63s/it]\u001b[A\n",
            "Iteration:  89%|████████████████████████████▍   | 8/9 [15:05<01:53, 113.32s/it]\u001b[A\n",
            "Iteration: 100%|████████████████████████████████| 9/9 [16:30<00:00, 110.03s/it]\u001b[A\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====Evaluation====\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating:   0%|                                        | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating:  12%|████                            | 1/8 [00:39<04:34, 39.27s/it]\u001b[A\n",
            "Evaluating:  25%|████████                        | 2/8 [01:19<03:57, 39.59s/it]\u001b[A\n",
            "Evaluating:  38%|████████████                    | 3/8 [02:00<03:19, 39.90s/it]\u001b[A\n",
            "Evaluating:  50%|████████████████                | 4/8 [02:40<02:40, 40.04s/it]\u001b[A\n",
            "Evaluating:  62%|████████████████████            | 5/8 [03:19<01:58, 39.63s/it]\u001b[A\n",
            "Evaluating:  75%|████████████████████████        | 6/8 [03:57<01:18, 39.23s/it]\u001b[A\n",
            "Evaluating:  88%|████████████████████████████    | 7/8 [04:35<00:38, 38.89s/it]\u001b[A\n",
            "Evaluating: 100%|████████████████████████████████| 8/8 [05:01<00:00, 37.75s/it]\u001b[A\n",
            "Epoch:  33%|██████████                    | 4/12 [1:33:16<3:10:50, 1431.33s/it]\n",
            "Iteration:   0%|                                         | 0/9 [00:00<?, ?it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluation:  {'loss': 0.5719665288925171, 'accuracy': 0.7154471544715447, 'f1_score': 0.7446174838294913, 'roc_auc': 0.7758838383838385}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration:  11%|███▌                            | 1/9 [01:51<14:52, 111.59s/it]\u001b[A\n",
            "Iteration:  22%|███████                         | 2/9 [03:42<12:59, 111.30s/it]\u001b[A\n",
            "Iteration:  33%|██████████▋                     | 3/9 [07:20<14:20, 143.45s/it]\u001b[A\n",
            "Iteration:  44%|██████████████▏                 | 4/9 [09:11<11:08, 133.62s/it]\u001b[A\n",
            "Iteration:  56%|█████████████████▊              | 5/9 [11:01<08:26, 126.63s/it]\u001b[A\n",
            "Iteration:  67%|█████████████████████▎          | 6/9 [12:49<06:03, 121.00s/it]\u001b[A\n",
            "Iteration:  78%|████████████████████████▉       | 7/9 [14:52<04:03, 121.51s/it]\u001b[A\n",
            "Iteration:  89%|████████████████████████████▍   | 8/9 [16:40<01:57, 117.63s/it]\u001b[A\n",
            "Iteration: 100%|████████████████████████████████| 9/9 [18:01<00:00, 120.19s/it]\u001b[A\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====Evaluation====\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating:   0%|                                        | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating:  12%|████                            | 1/8 [00:39<04:33, 39.06s/it]\u001b[A\n",
            "Evaluating:  25%|████████                        | 2/8 [01:16<03:52, 38.68s/it]\u001b[A\n",
            "Evaluating:  38%|████████████                    | 3/8 [01:55<03:13, 38.67s/it]\u001b[A\n",
            "Evaluating:  50%|████████████████                | 4/8 [02:33<02:34, 38.56s/it]\u001b[A\n",
            "Evaluating:  62%|████████████████████            | 5/8 [03:12<01:55, 38.61s/it]\u001b[A\n",
            "Evaluating:  75%|████████████████████████        | 6/8 [03:51<01:17, 38.59s/it]\u001b[A\n",
            "Evaluating:  88%|████████████████████████████    | 7/8 [04:30<00:38, 38.77s/it]\u001b[A\n",
            "Evaluating: 100%|████████████████████████████████| 8/8 [04:57<00:00, 37.14s/it]\u001b[A\n",
            "Epoch:  42%|████████████▌                 | 5/12 [1:56:18<2:45:17, 1416.81s/it]\n",
            "Iteration:   0%|                                         | 0/9 [00:00<?, ?it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluation:  {'loss': 0.4313420634716749, 'accuracy': 0.7886178861788617, 'f1_score': 0.8068736141906874, 'roc_auc': 0.8055555555555555}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration:  11%|███▌                            | 1/9 [01:52<14:56, 112.02s/it]\u001b[A\n",
            "Iteration:  22%|███████                         | 2/9 [03:39<12:54, 110.68s/it]\u001b[A\n",
            "Iteration:  33%|██████████▋                     | 3/9 [05:40<11:22, 113.69s/it]\u001b[A\n",
            "Iteration:  44%|██████████████▏                 | 4/9 [07:39<09:37, 115.42s/it]\u001b[A\n",
            "Iteration:  56%|█████████████████▊              | 5/9 [09:25<07:30, 112.58s/it]\u001b[A\n",
            "Iteration:  67%|█████████████████████▎          | 6/9 [11:15<05:34, 111.66s/it]\u001b[A\n",
            "Iteration:  78%|████████████████████████▉       | 7/9 [13:12<03:46, 113.34s/it]\u001b[A\n",
            "Iteration:  89%|████████████████████████████▍   | 8/9 [16:56<02:26, 146.61s/it]\u001b[A\n",
            "Iteration: 100%|████████████████████████████████| 9/9 [18:20<00:00, 122.25s/it]\u001b[A\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====Evaluation====\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating:   0%|                                        | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating:  12%|████                            | 1/8 [00:40<04:43, 40.53s/it]\u001b[A\n",
            "Evaluating:  25%|████████                        | 2/8 [01:20<04:01, 40.23s/it]\u001b[A\n",
            "Evaluating:  38%|████████████                    | 3/8 [01:58<03:18, 39.78s/it]\u001b[A\n",
            "Evaluating:  50%|████████████████                | 4/8 [02:37<02:37, 39.43s/it]\u001b[A\n",
            "Evaluating:  62%|████████████████████            | 5/8 [03:17<01:58, 39.55s/it]\u001b[A\n",
            "Evaluating:  75%|████████████████████████        | 6/8 [03:57<01:19, 39.62s/it]\u001b[A\n",
            "Evaluating:  88%|████████████████████████████    | 7/8 [04:36<00:39, 39.73s/it]\u001b[A\n",
            "Evaluating: 100%|████████████████████████████████| 8/8 [05:05<00:00, 38.13s/it]\u001b[A\n",
            "Epoch:  50%|███████████████               | 6/12 [2:19:50<2:21:30, 1415.11s/it]\n",
            "Iteration:   0%|                                         | 0/9 [00:00<?, ?it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluation:  {'loss': 0.5085053723305464, 'accuracy': 0.7560975609756098, 'f1_score': 0.7791811846689894, 'roc_auc': 0.7853535353535354}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration:  11%|███▌                            | 1/9 [01:46<14:15, 106.99s/it]\u001b[A\n",
            "Iteration:  22%|███████                         | 2/9 [03:36<12:33, 107.68s/it]\u001b[A\n",
            "Iteration:  33%|██████████▋                     | 3/9 [06:29<12:44, 127.37s/it]\u001b[A\n",
            "Iteration:  44%|██████████████▏                 | 4/9 [08:17<10:07, 121.55s/it]\u001b[A\n",
            "Iteration:  56%|█████████████████▊              | 5/9 [10:09<07:54, 118.52s/it]\u001b[A\n",
            "Iteration:  67%|█████████████████████▎          | 6/9 [11:58<05:47, 115.67s/it]\u001b[A\n",
            "Iteration:  78%|████████████████████████▉       | 7/9 [13:48<03:47, 113.97s/it]\u001b[A\n",
            "Iteration:  89%|████████████████████████████▍   | 8/9 [15:35<01:51, 111.96s/it]\u001b[A\n",
            "Iteration: 100%|████████████████████████████████| 9/9 [16:55<00:00, 112.82s/it]\u001b[A\n",
            "\n",
            "Evaluating:   0%|                                        | 0/8 [00:00<?, ?it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====Evaluation====\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating:  12%|████                            | 1/8 [00:38<04:29, 38.49s/it]\u001b[A\n",
            "Evaluating:  25%|████████                        | 2/8 [01:17<03:51, 38.53s/it]\u001b[A\n",
            "Evaluating:  38%|████████████                    | 3/8 [01:55<03:12, 38.50s/it]\u001b[A\n",
            "Evaluating:  50%|████████████████                | 4/8 [02:33<02:33, 38.38s/it]\u001b[A\n",
            "Evaluating:  62%|████████████████████            | 5/8 [03:12<01:55, 38.49s/it]\u001b[A\n",
            "Evaluating:  75%|████████████████████████        | 6/8 [03:50<01:16, 38.45s/it]\u001b[A\n",
            "Evaluating:  88%|████████████████████████████    | 7/8 [04:29<00:38, 38.48s/it]\u001b[A\n",
            "Evaluating: 100%|████████████████████████████████| 8/8 [04:55<00:00, 37.00s/it]\u001b[A\n",
            "Epoch:  58%|█████████████████▌            | 7/12 [2:41:42<1:55:21, 1384.37s/it]\n",
            "Iteration:   0%|                                         | 0/9 [00:00<?, ?it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluation:  {'loss': 0.4383882638067007, 'accuracy': 0.8048780487804879, 'f1_score': 0.8194848287893819, 'roc_auc': 0.7998737373737372}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration:  11%|███▌                            | 1/9 [02:54<23:18, 174.78s/it]\u001b[A\n",
            "Iteration:  22%|███████                         | 2/9 [04:45<18:09, 155.62s/it]\u001b[A\n",
            "Iteration:  33%|██████████▋                     | 3/9 [06:37<14:15, 142.58s/it]\u001b[A\n",
            "Iteration:  44%|██████████████▏                 | 4/9 [08:31<11:09, 133.88s/it]\u001b[A\n",
            "Iteration:  56%|█████████████████▊              | 5/9 [10:23<08:29, 127.44s/it]\u001b[A\n",
            "Iteration:  67%|█████████████████████▎          | 6/9 [12:15<06:07, 122.54s/it]\u001b[A\n",
            "Iteration:  78%|████████████████████████▉       | 7/9 [14:07<03:59, 119.51s/it]\u001b[A\n",
            "Iteration:  89%|████████████████████████████▍   | 8/9 [15:58<01:57, 117.03s/it]\u001b[A\n",
            "Iteration: 100%|████████████████████████████████| 9/9 [17:21<00:00, 115.72s/it]\u001b[A\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====Evaluation====\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating:   0%|                                        | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating:  12%|████                            | 1/8 [00:38<04:32, 38.88s/it]\u001b[A\n",
            "Evaluating:  25%|████████                        | 2/8 [01:17<03:53, 38.87s/it]\u001b[A\n",
            "Evaluating:  38%|████████████                    | 3/8 [01:56<03:14, 38.84s/it]\u001b[A\n",
            "Evaluating:  50%|████████████████                | 4/8 [02:35<02:35, 38.78s/it]\u001b[A\n",
            "Evaluating:  62%|████████████████████            | 5/8 [03:13<01:56, 38.74s/it]\u001b[A\n",
            "Evaluating:  75%|████████████████████████        | 6/8 [03:52<01:17, 38.67s/it]\u001b[A\n",
            "Evaluating:  88%|████████████████████████████    | 7/8 [04:30<00:38, 38.66s/it]\u001b[A\n",
            "Evaluating: 100%|████████████████████████████████| 8/8 [04:57<00:00, 37.17s/it]\u001b[A\n",
            "Epoch:  67%|████████████████████          | 8/12 [3:04:04<1:31:26, 1371.57s/it]\n",
            "Iteration:   0%|                                         | 0/9 [00:00<?, ?it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluation:  {'loss': 0.5357349291443825, 'accuracy': 0.7886178861788617, 'f1_score': 0.8068736141906874, 'roc_auc': 0.8055555555555555}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration:  11%|███▌                            | 1/9 [01:48<14:24, 108.12s/it]\u001b[A\n",
            "Iteration:  22%|███████                         | 2/9 [03:48<13:03, 111.88s/it]\u001b[A\n",
            "Iteration:  33%|██████████▋                     | 3/9 [06:13<12:08, 121.42s/it]\u001b[A\n",
            "Iteration:  44%|██████████████▏                 | 4/9 [08:04<09:52, 118.49s/it]\u001b[A\n",
            "Iteration:  56%|█████████████████▊              | 5/9 [09:58<07:48, 117.11s/it]\u001b[A\n",
            "Iteration:  67%|█████████████████████▎          | 6/9 [11:46<05:43, 114.44s/it]\u001b[A\n",
            "Iteration:  78%|████████████████████████▉       | 7/9 [13:45<03:51, 115.78s/it]\u001b[A\n",
            "Iteration:  89%|████████████████████████████▍   | 8/9 [15:34<01:53, 113.67s/it]\u001b[A\n",
            "Iteration: 100%|████████████████████████████████| 9/9 [16:55<00:00, 112.78s/it]\u001b[A\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====Evaluation====\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating:   0%|                                        | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating:  12%|████                            | 1/8 [00:38<04:32, 38.89s/it]\u001b[A\n",
            "Evaluating:  25%|████████                        | 2/8 [01:17<03:52, 38.69s/it]\u001b[A\n",
            "Evaluating:  38%|████████████                    | 3/8 [01:55<03:12, 38.51s/it]\u001b[A\n",
            "Evaluating:  50%|████████████████                | 4/8 [02:33<02:34, 38.55s/it]\u001b[A\n",
            "Evaluating:  62%|████████████████████            | 5/8 [03:12<01:55, 38.47s/it]\u001b[A\n",
            "Evaluating:  75%|████████████████████████        | 6/8 [03:51<01:17, 38.61s/it]\u001b[A\n",
            "Evaluating:  88%|████████████████████████████    | 7/8 [04:30<00:38, 38.84s/it]\u001b[A\n",
            "Evaluating: 100%|████████████████████████████████| 8/8 [04:58<00:00, 37.25s/it]\u001b[A\n",
            "Epoch:  75%|██████████████████████▌       | 9/12 [3:26:03<1:07:47, 1355.95s/it]\n",
            "Iteration:   0%|                                         | 0/9 [00:00<?, ?it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluation:  {'loss': 0.5508113512769341, 'accuracy': 0.8211382113821138, 'f1_score': 0.8332546551271965, 'roc_auc': 0.8099747474747474}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration:  11%|███▌                            | 1/9 [01:48<14:29, 108.73s/it]\u001b[A\n",
            "Iteration:  22%|███████                         | 2/9 [03:39<12:46, 109.47s/it]\u001b[A\n",
            "Iteration:  33%|██████████▋                     | 3/9 [05:31<11:00, 110.11s/it]\u001b[A\n",
            "Iteration:  44%|██████████████▏                 | 4/9 [07:31<09:25, 113.11s/it]\u001b[A\n",
            "Iteration:  56%|█████████████████▊              | 5/9 [09:23<07:30, 112.71s/it]\u001b[A\n",
            "Iteration:  67%|█████████████████████▎          | 6/9 [11:10<05:33, 111.06s/it]\u001b[A\n",
            "Iteration:  78%|████████████████████████▉       | 7/9 [13:08<03:46, 113.09s/it]\u001b[A\n",
            "Iteration:  89%|████████████████████████████▍   | 8/9 [15:02<01:53, 113.25s/it]\u001b[A\n",
            "Iteration: 100%|████████████████████████████████| 9/9 [16:20<00:00, 108.93s/it]\u001b[A\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====Evaluation====\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating:   0%|                                        | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating:  12%|████                            | 1/8 [00:38<04:31, 38.85s/it]\u001b[A\n",
            "Evaluating:  25%|████████                        | 2/8 [01:17<03:52, 38.76s/it]\u001b[A\n",
            "Evaluating:  38%|████████████                    | 3/8 [01:56<03:13, 38.78s/it]\u001b[A\n",
            "Evaluating:  50%|████████████████                | 4/8 [02:35<02:35, 38.96s/it]\u001b[A\n",
            "Evaluating:  62%|████████████████████            | 5/8 [03:14<01:56, 38.82s/it]\u001b[A\n",
            "Evaluating:  75%|████████████████████████        | 6/8 [03:52<01:17, 38.69s/it]\u001b[A\n",
            "Evaluating:  88%|████████████████████████████    | 7/8 [04:31<00:38, 38.66s/it]\u001b[A\n",
            "Evaluating: 100%|████████████████████████████████| 8/8 [04:57<00:00, 37.21s/it]\u001b[A\n",
            "Epoch:  83%|█████████████████████████▊     | 10/12 [3:47:27<44:28, 1334.35s/it]\n",
            "Iteration:   0%|                                         | 0/9 [00:00<?, ?it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluation:  {'loss': 0.6081431629136205, 'accuracy': 0.7642276422764228, 'f1_score': 0.7869225106737989, 'roc_auc': 0.8061868686868687}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration:  11%|███▌                            | 1/9 [01:47<14:22, 107.79s/it]\u001b[A\n",
            "Iteration:  22%|███████                         | 2/9 [03:38<12:39, 108.55s/it]\u001b[A\n",
            "Iteration:  33%|██████████▋                     | 3/9 [05:28<10:55, 109.20s/it]\u001b[A\n",
            "Iteration:  44%|██████████████▏                 | 4/9 [07:19<09:08, 109.74s/it]\u001b[A\n",
            "Iteration:  56%|█████████████████▊              | 5/9 [09:13<07:23, 110.82s/it]\u001b[A\n",
            "Iteration:  67%|█████████████████████▎          | 6/9 [11:04<05:32, 110.94s/it]\u001b[A\n",
            "Iteration:  78%|████████████████████████▉       | 7/9 [13:14<03:52, 116.39s/it]\u001b[A\n",
            "Iteration:  89%|████████████████████████████▍   | 8/9 [15:24<02:00, 120.77s/it]\u001b[A\n",
            "Iteration: 100%|████████████████████████████████| 9/9 [16:47<00:00, 111.97s/it]\u001b[A\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====Evaluation====\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating:   0%|                                        | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating:  12%|████                            | 1/8 [00:39<04:37, 39.61s/it]\u001b[A\n",
            "Evaluating:  25%|████████                        | 2/8 [01:18<03:55, 39.32s/it]\u001b[A\n",
            "Evaluating:  38%|████████████                    | 3/8 [01:57<03:16, 39.21s/it]\u001b[A\n",
            "Evaluating:  50%|████████████████                | 4/8 [02:37<02:37, 39.49s/it]\u001b[A\n",
            "Evaluating:  62%|████████████████████            | 5/8 [03:17<01:58, 39.57s/it]\u001b[A\n",
            "Evaluating:  75%|████████████████████████        | 6/8 [03:57<01:19, 39.77s/it]\u001b[A\n",
            "Evaluating:  88%|████████████████████████████    | 7/8 [04:35<00:39, 39.24s/it]\u001b[A\n",
            "Evaluating: 100%|████████████████████████████████| 8/8 [05:03<00:00, 37.88s/it]\u001b[A\n",
            "Epoch:  92%|████████████████████████████▍  | 11/12 [4:09:21<22:08, 1328.01s/it]\n",
            "Iteration:   0%|                                         | 0/9 [00:00<?, ?it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluation:  {'loss': 0.5669467113912106, 'accuracy': 0.8130081300813008, 'f1_score': 0.8263635086505938, 'roc_auc': 0.8049242424242424}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration:  11%|███▌                            | 1/9 [01:48<14:26, 108.28s/it]\u001b[A\n",
            "Iteration:  22%|███████                         | 2/9 [03:41<12:48, 109.82s/it]\u001b[A\n",
            "Iteration:  33%|██████████▋                     | 3/9 [05:29<10:54, 109.10s/it]\u001b[A\n",
            "Iteration:  44%|██████████████▏                 | 4/9 [08:57<11:34, 138.89s/it]\u001b[A\n",
            "Iteration:  56%|█████████████████▊              | 5/9 [10:50<08:44, 131.00s/it]\u001b[A\n",
            "Iteration:  67%|█████████████████████▎          | 6/9 [12:42<06:16, 125.41s/it]\u001b[A\n",
            "Iteration:  78%|████████████████████████▉       | 7/9 [14:30<04:00, 120.07s/it]\u001b[A\n",
            "Iteration:  89%|████████████████████████████▍   | 8/9 [16:20<01:57, 117.06s/it]\u001b[A\n",
            "Iteration: 100%|████████████████████████████████| 9/9 [17:45<00:00, 118.39s/it]\u001b[A\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====Evaluation====\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating:   0%|                                        | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating:  12%|████                            | 1/8 [00:40<04:40, 40.07s/it]\u001b[A\n",
            "Evaluating:  25%|████████                        | 2/8 [01:20<04:00, 40.05s/it]\u001b[A\n",
            "Evaluating:  38%|████████████                    | 3/8 [02:00<03:20, 40.17s/it]\u001b[A\n",
            "Evaluating:  50%|████████████████                | 4/8 [02:40<02:40, 40.20s/it]\u001b[A\n",
            "Evaluating:  62%|████████████████████            | 5/8 [03:21<02:00, 40.31s/it]\u001b[A\n",
            "Evaluating:  75%|████████████████████████        | 6/8 [04:01<01:20, 40.37s/it]\u001b[A\n",
            "Evaluating:  88%|████████████████████████████    | 7/8 [04:42<00:40, 40.33s/it]\u001b[A\n",
            "Evaluating: 100%|████████████████████████████████| 8/8 [05:10<00:00, 38.80s/it]\u001b[A\n",
            "Epoch: 100%|███████████████████████████████| 12/12 [4:32:18<00:00, 1361.58s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluation:  {'loss': 0.5488417046144605, 'accuracy': 0.8211382113821138, 'f1_score': 0.8318003686273107, 'roc_auc': 0.7941919191919191}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXPYUtsQ62oA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}