{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code converts text to files with tokens: train and apply SentencePiece library, preprocessing and tokenize text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from zipfile import ZipFile\n",
    "import os\n",
    "import csv\n",
    "import re\n",
    "import sentencepiece as spm\n",
    "import time\n",
    "import logging\n",
    "import emoji\n",
    "import json\n",
    "import copy\n",
    "\n",
    "from spm_train import spm_train\n",
    "from spm_apply import spm_apply\n",
    "from data_convert import clean_input_text, convert_into_train_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"max_colwidth\", 150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114239\n"
     ]
    }
   ],
   "source": [
    "list_of_files = os.listdir(\"dataset/\")\n",
    "text_file = []\n",
    "for file in list_of_files:\n",
    "    with open(\"dataset/\" + file, \"r\", encoding=\"utf-8\") as train:\n",
    "        reader = csv.reader(train, delimiter=\"\\t\")\n",
    "        for line in reader:\n",
    "            text_file.append(line)\n",
    "            \n",
    "print(len(text_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the condition that the length of the sentence must be at least 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "for i in text_file:\n",
    "    if i != []:\n",
    "        for j in i:\n",
    "            if len(j) > 30:\n",
    "                texts.append(j)\n",
    "text_df = pd.DataFrame(texts, columns = ['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean the texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_input_text(df, column_name: str):\n",
    "    train_df = pd.DataFrame(columns=[column_name])\n",
    "    train_df[column_name] = df[column_name]\n",
    "    assert len(train_df) == len(df)\n",
    "    text_list = []\n",
    "    for text in train_df[column_name]:\n",
    "#         text_clean = re.compile(r'[.+-,!\"*#$%&’(),/:;<=>?@/\\^_`-·]')\n",
    "#         num = re.compile(r'\\d+')\n",
    "\n",
    "#         emoji_pattern = re.compile(\"[\"\n",
    "#                                    u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "#                                    u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "#                                    u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "#                                    u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "#                                    u\"\\U00002702-\\U000027B0\"\n",
    "#                                    u\"\\U000024C2-\\U0001F251\"\n",
    "#                                    \"]+\", flags=re.UNICODE)\n",
    "\n",
    "#         allchars = [str for str in text]\n",
    "#         emoji_list = [c for c in allchars if c in emoji.UNICODE_EMOJI]\n",
    "#         clean_text = ' '.join([str for str in text.split() if not any(i in str for i in emoji_list)])\n",
    "\n",
    "#         new_text = emoji_pattern.sub(r'', clean_text)\n",
    "#         cln_text = text_clean.sub(r'', new_text)\n",
    "#         cl_text = num.sub(r'', cln_text)\n",
    "#         c_text = cl_text.replace('/', '')\n",
    "        low_text = text.lower()\n",
    "\n",
    "        text_list.append(str(low_text))\n",
    "    return text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text = clean_input_text(text_df, column_name = 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(92359,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text_arr = np.array(clean_text)\n",
    "clean_text_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text_array = np.reshape(clean_text_arr, (len(clean_text_arr),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wtire a file for training\n",
    "with open('spm_train_file.txt', \"w\", encoding='utf-8') as output:\n",
    "    for line in clean_text_array:\n",
    "        output.write(\" \".join(line) + \"\\n\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train SentencePiece (BPE) on the additional dataset for different ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADDITIONAL_SPECIAL_TOKENS = \"<e1>\", \"</e1>\", \"<e2>\", \"</e2>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spm_train(input_file = 'spm_train_file.txt', model_type = 'bpe', vocab_size = '10000', model_prefix = 'model_bpe_dip_10000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spm_train(input_file = 'spm_train_file.txt', model_type = 'bpe', vocab_size = '30000', model_prefix = 'model_bpe_dip_30000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: model_bpe_dip_50000\n",
      "Time: 24.254483222961426\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The model was trained saccessfuly'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spm_train(input_file = 'spm_train_file.txt', model_type = 'bpe', vocab_size = '50000', model_prefix = 'model_bpe_dip_50000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_examples_from_file(data_dir):\n",
    "    #file_path = os.path.join(data_dir, \"{}.txt\".format(mode))\n",
    "    guid_index = 1\n",
    "    examples = dict()\n",
    "    texts = []\n",
    "    labels = []\n",
    "    with open(data_dir, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f.readlines():\n",
    "            line = line.strip().split(\"\\t\")\n",
    "            if len(line) == 2:\n",
    "                text_a = line[0]\n",
    "                label = line[1]\n",
    "            else:\n",
    "                text_a = line[0]\n",
    "                label = \"NONE\"\n",
    "            texts.append(text_a)\n",
    "            labels.append(label)\n",
    "            guid_index += 1\n",
    "    assert len(labels) == len(texts)\n",
    "\n",
    "    return texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = read_examples_from_file('data/train_balanced.csv')\n",
    "eval_data = read_examples_from_file('data/eval_balanced.csv')\n",
    "test_data = read_examples_from_file('data/test_file.csv')\n",
    "labels = [0,1]\n",
    "num_labels = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text, train_labels = train_data\n",
    "train_text = pd.DataFrame(train_text, columns = ['text'])\n",
    "train_text_ = clean_input_text(train_text, column_name = 'text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization and vectorization with SentencePiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 0.08305883407592773\n",
      "List with subword was written successfuly!\n",
      "Len of input_text: 140\n",
      "Len of subword_tokens_list: 140\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_text, train_labels = train_data\n",
    "\n",
    "\n",
    "train_tokens, train_vectors = spm_apply(train_text, 'model_bpe_dip.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 0.04002809524536133\n",
      "List with subword was written successfuly!\n",
      "Len of input_text: 123\n",
      "Len of subword_tokens_list: 123\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eval_text, eval_labels = eval_data\n",
    "\n",
    "eval_tokens, eval_vectors = spm_apply(eval_text, 'model_bpe_dip.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 0.02601766586303711\n",
      "List with subword was written successfuly!\n",
      "Len of input_text: 74\n",
      "Len of subword_tokens_list: 74\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_text, test_labels = test_data\n",
    "\n",
    "test_tokens, test_vectors = spm_apply(test_text, 'model_bpe_dip.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁', '<e1>', 'система', '</e1>', '▁предназначена', '▁для', '▁', '<e2>', 'анализа', '▁множества', '▁ген', 'ом', 'ов', '</e2>', '▁и', '▁их', '▁классификации', ',', '▁а', '▁именно', '▁для', '▁анализа', '▁ген', 'оти', 'пов', '▁внутри', '▁одного', '▁вида', '▁жи', 'вых', '▁органи', 'з', 'мов', ',', '▁поскольку', '▁методы', '▁направл', 'ены', '▁для', '▁выделения', '▁различий', '▁ген', 'ом', 'ов', ',', '▁имеющих', '▁схо', 'ж', 'ую', '▁структуру', '.']\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(train_tokens[11])\n",
    "print(type(train_vectors[11]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pulling words out of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_vectors(data_vectors, vectors_ot_tokens):\n",
    "    f_w = []\n",
    "    s_w = []\n",
    "    start1 = []\n",
    "    end1 = []\n",
    "    start2 = []\n",
    "    end2 = []\n",
    "    for idx, row in enumerate(data_vectors):\n",
    "        for i,ex in enumerate(row):\n",
    "            if ex == 3:\n",
    "                start1.append(i)\n",
    "            elif ex == 4:\n",
    "                end1.append(i)\n",
    "            elif ex == 5:\n",
    "                start2.append(i)\n",
    "            elif ex == 6:\n",
    "                end2.append(i)\n",
    "\n",
    "        first_word = []\n",
    "        second_word = []\n",
    "\n",
    "        for i in range(start1[idx]+1, end1[idx]):\n",
    "            first_word.append(vectors_ot_tokens[idx][i])\n",
    "\n",
    "        for j in range(start2[idx]+1, end2[idx]):\n",
    "            second_word.append(vectors_ot_tokens[idx][j])\n",
    "\n",
    "        f_w.append(first_word)\n",
    "        s_w.append(second_word)\n",
    "    return f_w, s_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_w_vectors_train, s_w_vectors_train = get_words_vectors(train_vectors, train_vectors)\n",
    "f_w_tokens_train, s_w_tokens_train = get_words_vectors(train_vectors, train_tokens)\n",
    "\n",
    "f_w_vectors_eval, s_w_vectors_eval = get_words_vectors(eval_vectors, eval_vectors)\n",
    "f_w_tokens_eval, s_w_tokens_eval = get_words_vectors(eval_vectors, eval_tokens)\n",
    "\n",
    "f_w_vectors_test, s_w_vectors_test = get_words_vectors(test_vectors, test_vectors)\n",
    "f_w_tokens_test, s_w_tokens_test = get_words_vectors(test_vectors, test_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_df = pd.DataFrame(train_text, columns = ['text'])\n",
    "train_text_df['label'] = train_labels\n",
    "train_text_df['tokens'] = train_tokens\n",
    "train_text_df['vectors'] = train_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_text_df = pd.DataFrame(eval_text, columns = ['text'])\n",
    "eval_text_df['label'] = eval_labels\n",
    "eval_text_df['tokens'] = eval_tokens\n",
    "eval_text_df['vectors'] = eval_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text_df = pd.DataFrame(test_text, columns = ['text'])\n",
    "test_text_df['label'] = test_labels\n",
    "test_text_df['tokens'] = test_tokens\n",
    "test_text_df['vectors'] = test_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_df.to_csv('train_data_with_vectors_5.csv', sep = ',', index = False)\n",
    "eval_text_df.to_csv('eval_data_with_vectors_5.csv', sep = ',', index = False)\n",
    "test_text_df.to_csv('test_data_with_vectors_3.csv', sep = ',', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split words (first and second) with Is-A relations and Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "isa_f = []\n",
    "nonisa_f = []\n",
    "for i,j in zip(f_w_tokens_eval,eval_labels):\n",
    "    if j == '1':\n",
    "        isa_f.append(i)\n",
    "    else:\n",
    "        nonisa_f.append(i)\n",
    "\n",
    "new_f = []\n",
    "for i in isa_f:\n",
    "    a = \",\".join(i).replace(',', '').replace('▁',' ')\n",
    "    new_f.append([a])\n",
    "    \n",
    "for i in nonisa_f:\n",
    "    a = \",\".join(i).replace(',', '').replace('▁',' ')\n",
    "    new_f.append([a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "isa_s = []\n",
    "nonisa_s = []\n",
    "for i,j in zip(s_w_tokens_eval,eval_labels):\n",
    "    if j == '1':\n",
    "        isa_s.append(i)\n",
    "    else:\n",
    "        nonisa_s.append(i)\n",
    "        \n",
    "new_s = []\n",
    "for i in isa_s:\n",
    "    a = \",\".join(i).replace(',', '').replace('▁',' ')\n",
    "    new_s.append([a])\n",
    "    \n",
    "for i in nonisa_s:\n",
    "    a = \",\".join(i).replace(',', '').replace('▁',' ')\n",
    "    new_s.append([a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_f = np.array(new_f)\n",
    "new_s = np.array(new_s)\n",
    "dsf = pd.DataFrame(new_f, columns = ['f'])\n",
    "dsf['s'] = new_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsf.to_csv('words_eval.csv', sep = ',', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_words(f_w, s_w):\n",
    "    f_w_arr = np.array(f_w)\n",
    "    s_w_arr = np.array(s_w)\n",
    "    first_words = np.reshape(f_w_arr, (len(f_w),1))\n",
    "    second_words = np.reshape(s_w_arr, (len(s_w),1))\n",
    "    words_vectors_df = pd.DataFrame(first_words, columns = ['f_w'])\n",
    "    words_vectors_df['s_w'] = second_words\n",
    "    return words_vectors_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\envs\\venv\\lib\\site-packages\\ipykernel_launcher.py:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  \n",
      "C:\\Users\\admin\\anaconda3\\envs\\venv\\lib\\site-packages\\ipykernel_launcher.py:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "words_vectors_train = df_words(f_w_vectors_train, s_w_vectors_train)\n",
    "words_vectors_eval = df_words(f_w_vectors_eval, s_w_vectors_eval)\n",
    "words_vectors_test = df_words(f_w_vectors_test, s_w_vectors_test)\n",
    "\n",
    "words_tokens_train = df_words(f_w_tokens_train, s_w_tokens_train)\n",
    "words_tokens_eval = df_words(f_w_tokens_eval, s_w_tokens_eval)\n",
    "words_tokens_test = df_words(f_w_tokens_test, s_w_tokens_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_vectors_train.to_csv('words_vectors_train.csv', sep = ',', index = False)\n",
    "words_vectors_eval.to_csv('words_vectors_eval.csv', sep = ',', index = False)\n",
    "words_vectors_test.to_csv('words_vectors_test.csv', sep = ',', index = False)\n",
    "\n",
    "words_tokens_train.to_csv('words_tokens_train.csv', sep = ',', index = False)\n",
    "words_tokens_eval.to_csv('words_tokens_eval.csv', sep = ',', index = False)\n",
    "words_tokens_test.to_csv('words_tokens_test.csv', sep = ',', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
